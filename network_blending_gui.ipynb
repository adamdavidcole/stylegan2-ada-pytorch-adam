{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "network_blending_gui.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "TgF1sYKLFYRv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamdavidcole/stylegan2-ada-pytorch-adam/blob/main/network_blending_gui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdkWE1TKUWwB"
      },
      "source": [
        "# Network Blending\n",
        "This demo will show how to combine two separate StyleGAN2-ADA-PyTorch models into one by splitting their weights at a specified layer.\n",
        "\n",
        "This example was created by Derrick Schultz for his Advanced StyleGAN2 class. It’s a simpler version of [Justin Pinkney’s Tensorflow version](https://github.com/justinpinkney/stylegan2/blob/master/blend_models.py).\n",
        "\n",
        "---\n",
        "\n",
        "If you find this notebook useful, consider signing up for my [Patreon](https://www.patreon.com/bustbright) or [YouTube channel](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA/join). You can also send me a one-time payment on [Venmo](https://venmo.com/Derrick-Schultz).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJFXX8WIBeqy"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops ninja gdown"
      ],
      "metadata": {
        "id": "WtrPIFTpz4OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive \n",
        "# (NOTE: only run this if you want to save the results in GDrive after the runtime ends)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gNwryEcAy2Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip install gdown --upgrade\n",
        "\n",
        "if os.path.isdir(\"/content/drive/MyDrive/stylegan2-ada-pytorch-adam\"):\n",
        "    %cd \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !git clone https://github.com/adamdavidcole/stylegan2-ada-pytorch-adam.git\n",
        "    %cd stylegan2-ada-pytorch-adam\n",
        "    \n",
        "    # !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "else:\n",
        "    !git clone https://github.com/adamdavidcole/stylegan2-ada-pytorch-adam.git\n",
        "    %cd stylegan2-ada-pytorch-adam\n",
        "    # !mkdir downloads\n",
        "    # !mkdir datasets\n",
        "    # !mkdir pretrained\n",
        "\n",
        "!mkdir input_images\n",
        "!mkdir input_images/raw\n",
        "!mkdir input_images/aligned"
      ],
      "metadata": {
        "id": "Ypk368bp0rPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "!git fetch origin\n",
        "# !git pull\n",
        "# !git stash\n",
        "!git checkout origin/main -- \"*.py\" \n",
        "# !git checkout origin/main -- \"*.ipynb\"\n",
        "# !git checkout origin/main -- \"ffhq_dataset/*\" \n"
      ],
      "metadata": {
        "id": "wYPAO7rBV42E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja opensimplex"
      ],
      "metadata": {
        "id": "uIzGlcV21Ms_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python legacy.py \\\n",
        "        --source=/content/drive/MyDrive/stylegan2-ada-pytorch-adam/pretrained/stylegan2-ffhq-slim.pkl.txt \\\n",
        "        --dest=/content/drive/MyDrive/stylegan2-ada-pytorch-adam/pretrained/stylegan2-ffhq-slim.pkl"
      ],
      "metadata": {
        "id": "JFnbARnActDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O09iXoAPQsHX"
      },
      "source": [
        "## Download Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRQUzZK2HHSt"
      },
      "source": [
        "# https://drive.google.com/file/d//view?usp=sharing\n",
        "if not os.path.isdir('pretrained'):\n",
        "  !mkdir pretrained\n",
        "\n",
        "# butterflys\n",
        "!gdown --id 105VsQSTdthX4lSvHUW6YM0_MiaHcOruJ -O pretrained/butterflys_000016.pkl\n",
        "!gdown --id 107MmrDtr0GX8rDXDJzi59-7tnlz0QhjC -O pretrained/butterflys_000032.pkl\n",
        "!gdown --id 10QXjGYAo9sn-UYKKpJqyDX1ON14OscQ2 -O pretrained/butterflys_000048.pkl\n",
        "!gdown --id 15NC-plFvfs59NLT0-t3SIucpJcEmvOmq -O pretrained/butterflys_000677.pkl\n",
        "\n",
        "# ukiyoe\n",
        "!gdown --id 1BkRsnE0YygA2ufbfDOV4-fOgTMjSr94K -O pretrained/stylegan2-ffhq-slim.pkl\n",
        "!gdown --id 1BjYGiOUKk8SC35a2e5QrJ1QtvaxJ0QD7 -O pretrained/ukiyoe-256-slim.pkl\n",
        "\n",
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl -O pretrained/ffhq_256.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions & Setup\n",
        "#common functions \n",
        "import pickle, torch, PIL, copy, cv2, math\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "from IPython.display import Image as DisplayImage, clear_output\n",
        "\n",
        "# define device to use\n",
        "device = torch.device('cuda')\n",
        "\n",
        "def get_model(path):\n",
        "  # with open(path, 'rb') as f:\n",
        "  #   _G = pickle.load(f)['G_ema'].cuda()\n",
        "  device = torch.device('cuda')\n",
        "  with dnnlib.util.open_url(path) as fp:\n",
        "      _G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)\n",
        "  \n",
        "  return _G\n",
        "\n",
        "#tensor to PIL image \n",
        "def t2i(t):\n",
        "  return PIL.Image.fromarray((t*127.5+127).clamp(0,255)[0].permute(1,2,0).cpu().numpy().astype('uint8'))\n",
        "\n",
        "#stack an array of PIL images horizontally\n",
        "def add_imgs(images):\n",
        "  widths, heights = zip(*(i.size for i in images))\n",
        "\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    new_im.paste(im, (x_offset,0))\n",
        "    x_offset += im.size[0]\n",
        "  return new_im\n",
        "\n",
        "\n",
        "def apply_mask(matrix, mask, fill_value):\n",
        "    masked = np.ma.array(matrix, mask=mask, fill_value=fill_value)\n",
        "    return masked.filled()\n",
        " \n",
        "def apply_threshold(matrix, low_value, high_value):\n",
        "    low_mask = matrix < low_value\n",
        "    matrix = apply_mask(matrix, low_mask, low_value)\n",
        " \n",
        "    high_mask = matrix > high_value\n",
        "    matrix = apply_mask(matrix, high_mask, high_value)\n",
        " \n",
        "    return matrix\n",
        "\n",
        "# A simple color correction script to brighten overly dark images\n",
        "def simplest_cb(img, percent):\n",
        "    assert img.shape[2] == 3\n",
        "    assert percent > 0 and percent < 100\n",
        " \n",
        "    half_percent = percent / 200.0\n",
        " \n",
        "    channels = cv2.split(img)\n",
        " \n",
        "    out_channels = []\n",
        "    for channel in channels:\n",
        "        assert len(channel.shape) == 2\n",
        "        # find the low and high precentile values (based on the input percentile)\n",
        "        height, width = channel.shape\n",
        "        vec_size = width * height\n",
        "        flat = channel.reshape(vec_size)\n",
        " \n",
        "        assert len(flat.shape) == 1\n",
        " \n",
        "        flat = np.sort(flat)\n",
        " \n",
        "        n_cols = flat.shape[0]\n",
        " \n",
        "        low_val  = flat[math.floor(n_cols * half_percent)-1]\n",
        "        high_val = flat[math.ceil( n_cols * (1.0 - half_percent))-1]\n",
        " \n",
        " \n",
        "        # saturate below the low percentile and above the high percentile\n",
        "        thresholded = apply_threshold(channel, low_val, high_val)\n",
        "        # scale the channel\n",
        "        normalized = cv2.normalize(thresholded, thresholded.copy(), 0, 255, cv2.NORM_MINMAX)\n",
        "        out_channels.append(normalized)\n",
        " \n",
        "    return cv2.merge(out_channels)\n",
        " \n",
        "def normalize(inf, thresh):\n",
        "    img = np.array(inf)\n",
        "    out_img = simplest_cb(img, thresh)\n",
        "    return PIL.Image.fromarray(out_img)\n",
        "\n",
        "def get_w_from_path(w_path):\n",
        "  projected_w_np = np.load(projected_w_path)[0]\n",
        "  w = torch.tensor(projected_w_np).to(device).unsqueeze(0)\n",
        "  return w\n",
        "\n",
        "def synthesize_tensor_from_w(G, w):\n",
        "  # print(w.shape)\n",
        "  # print(w)\n",
        "  return G.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "\n",
        "def synthesize_img_from_w(G, w):\n",
        "  tensor = synthesize_tensor_from_w(G, w)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_tensor_from_w_path(G, w_path):\n",
        "  w = get_w_from_path(w_path)\n",
        "  return synthesize_tensor_from_w(G, w)\n",
        "\n",
        "def synthesize_img_from_w_path(G, w_path):\n",
        "  tensor = synthesize_tensor_from_w_path(G, w_path)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_img_from_w_path(G, w_path):\n",
        "  tensor = synthesize_tensor_from_w_path(G, w_path)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_img_from_w_np(G, w_np):\n",
        "  w = torch.tensor(w_np).to(device).unsqueeze(0)\n",
        "  tensor = synthesize_img_from_w(G, w)\n",
        "  return t2i(tensor)\n",
        "\n",
        "\n",
        "\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UpQmiH8zJlHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENNGfqvKS59e",
        "cellView": "form"
      },
      "source": [
        "#@title Blend Functions\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "def extract_conv_names(model, model_res):\n",
        "    model_names = list(name for name,weight in model.named_parameters())\n",
        "\n",
        "    return model_names\n",
        "\n",
        "def blend_models(low, high, model_res, resolution, level, blend_width=None, blend_mask=None):\n",
        "\n",
        "    resolutions =  [4*2**x for x in range(int(np.log2(resolution)-1))]\n",
        "    \n",
        "    low_names = extract_conv_names(low, model_res)\n",
        "    high_names = extract_conv_names(high, model_res)\n",
        "\n",
        "    assert all((x == y for x, y in zip(low_names, high_names)))\n",
        "\n",
        "    #start with lower model and add weights above\n",
        "    model_out = copy.deepcopy(low)\n",
        "    params_src = high.named_parameters()\n",
        "    dict_dest = model_out.state_dict()\n",
        "\n",
        "    if blend_mask is None:\n",
        "      for name, param in params_src:\n",
        "          if not any(f'synthesis.b{res}' in name for res in resolutions) and not ('mapping' in name):\n",
        "              # print(name)\n",
        "              # print(param.data)\n",
        "              dict_dest[name].data.copy_(param.data)\n",
        "    else:\n",
        "      for name, param in params_src:\n",
        "        if not ('mapping' in name):\n",
        "          # print(f\"name: {name}\")\n",
        "\n",
        "\n",
        "          for idx, res in enumerate(resolutions):\n",
        "            if f'synthesis.b{res}' in name:\n",
        "              mask_val = blend_mask[idx]\n",
        "              next_data = dict_dest[name].data * (1 - mask_val) + param.data * (mask_val)\n",
        "\n",
        "              # print(mask_val)\n",
        "\n",
        "              dict_dest[name].data.copy_(next_data)\n",
        "\n",
        "\n",
        "    model_out_dict = model_out.state_dict()\n",
        "    model_out_dict.update(dict_dest) \n",
        "    model_out.load_state_dict(dict_dest)\n",
        "    \n",
        "    return model_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Models {run: \"auto\"}\n",
        "#@markdown Select a pretrained model for the source and destination or paste links to your own\n",
        "#@markdown <br/>(Note: destination must be fine-tuned from source and both must be StyleGAN2 pkl format)\n",
        "source_model = \"FFHQ_256\" #@param [\"FFHQ_256\", \"FFHQ_256_slim\"] {allow-input: true}\n",
        "destination_model = \"Butteflys_0048\" #@param [\"Butteflys_0016\", \"Butteflys_0032\", \"Butteflys_0048\", \"Butteflys_0677\", \"Ukiyoe_256_slim\"] {allow-input: true}\n",
        "\n",
        "\n",
        "model_keys = {\n",
        "    \"Butteflys_0016\": \"pretrained/butterflys_000016.pkl\",\n",
        "    \"Butteflys_0032\": \"pretrained/butterflys_000032.pkl\",\n",
        "    \"Butteflys_0048\": \"pretrained/butterflys_000048.pkl\",\n",
        "    \"Butteflys_0677\": \"pretrained/butterflys_000677.pkl\",\n",
        "    \"FFHQ_256\": \"pretrained/ffhq_256.pkl\", \n",
        "\n",
        "    \"FFHQ_256_slim\": \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/pretrained/stylegan2-ffhq-slim.pkl\",\n",
        "    \"Ukiyoe_256_slim\": \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/pretrained/ukiyoe-256-slim.pkl\"\n",
        "}\n",
        "\n",
        "lo_res_pkl = model_keys[source_model] if source_model in model_keys  else source_model\n",
        "hi_res_pkl = model_keys[destination_model] if destination_model in model_keys else destination_model\n",
        "model_res = 256\n",
        "level = 0\n",
        "blend_width=None\n",
        "\n",
        "G_kwargs = dnnlib.EasyDict()\n",
        "\n",
        "with dnnlib.util.open_url(lo_res_pkl) as f:\n",
        "    # G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "    lo = legacy.load_network_pkl(f, custom=False, **G_kwargs) # type: ignore\n",
        "    lo_G, lo_D, lo_G_ema = lo['G'], lo['D'], lo['G_ema']\n",
        "\n",
        "with dnnlib.util.open_url(hi_res_pkl) as f:\n",
        "    # G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "    hi = legacy.load_network_pkl(f, custom=False, **G_kwargs)['G_ema'] # type: ignore\n",
        "    #hi_G, hi_D, hi_G_ema = hi['G'], lo['D'], lo['G_ema']\n"
      ],
      "metadata": {
        "id": "hQbd5bciK4AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Face"
      ],
      "metadata": {
        "id": "ypk8JJhaE5au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nqo0U92Ct013"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python align_images.py "
      ],
      "metadata": {
        "id": "MxHR8i2cUVOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_network=\n",
        "\n",
        "!python train.py --outdir=results --data=/content/drive/MyDrive/stylegan2-ada-pytorch-adam/input_images/raw/pokemon_256.zip \\\n",
        "  --gpus=1 --cfg=paper256 --mirror=1 --snap=1 --aug=noaug --metrics=none --resume=$lo_res_pkl\n"
      ],
      "metadata": {
        "id": "t77-nu_Ku58u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "# ts stores the time in seconds\n",
        "ts = int(time.time())\n",
        "\n",
        "network_name = os.path.basename(lo_res_pkl)\n",
        "projection_outdir = f\"projections/{ts}_{network_name}/\"\n",
        "\n",
        "num_steps = 1001 #@param {type: \"slider\", min: 1, max: 10000, step: 1}\n",
        "uploaded_file_path = \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/input_images/aligned/mona_real_01.png\" #@param {type: \"string\"} \n",
        "\n",
        "!python projector.py --outdir=$projection_outdir --target=$uploaded_file_path --num-steps=$num_steps --save-video=false \\\n",
        "  --network=$lo_res_pkl"
      ],
      "metadata": {
        "id": "u_kr75qaE8xz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network Blend Basic"
      ],
      "metadata": {
        "id": "TgF1sYKLFYRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Blend Layer {run: \"auto\"}\n",
        "device = \"cuda\"\n",
        "\n",
        "#@markdown **Select source vector**\n",
        "projected_w_path = \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/projections/p1655515028/projected_w.npz\" #@param {type: \"string\"}\n",
        "use_projected_w = False #@param {type:\"boolean\"}\n",
        "seed=5601 #@param {type: \"slider\", min: 0, max: 10000, step: 1}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "switch_layer = 8 #@param [4, 8, 16, 32, 64, 128]  {type:\"raw\"}\n",
        "blend_width = 0 #@param {type: \"slider\", min: 0, max: 5, step: 0.01}\n",
        "model_out = blend_models(lo_G_ema, hi, model_res, switch_layer, level, blend_width=blend_width)\n",
        "\n",
        "G1 = lo_G_ema.to(device)\n",
        "G2 = hi.to(device)\n",
        "G_blend = model_out.to(device)\n",
        "\n",
        "\n",
        "if use_projected_w:\n",
        "  w_np = np.load(projected_w_path)['w']\n",
        "  w = torch.tensor(w_np).to(device)\n",
        "else:\n",
        "  label = torch.zeros([1, G1.c_dim], device=device)\n",
        "  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G1.z_dim)).to(device)\n",
        "\n",
        "  w = G1.mapping(z, None, truncation_psi=0.8, truncation_cutoff=8)\n",
        "\n",
        "\n",
        "g1_img = G1.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g1_img = (g1_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g1_imgfile = PIL.Image.fromarray(g1_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "# g1_imgfile.save(f'G1seed{seed:04d}.png')\n",
        "g2_img = G2.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g2_img = (g2_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g2_imgfile = PIL.Image.fromarray(g2_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "g3_img = G_blend.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g3_img = (g3_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g3_imgfile = PIL.Image.fromarray(g3_img[0].cpu().numpy(), 'RGB')\n",
        "display(add_imgs([g1_imgfile, g3_imgfile, g2_imgfile]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1OHi3Tm5of-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tune Blend"
      ],
      "metadata": {
        "id": "0mcnplXLoMkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Blend Layer {run: \"auto\"}\n",
        "device = \"cuda\"\n",
        "\n",
        "#@markdown **Select source vector**\n",
        "projected_w_path = \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/projections/1655519505_ffhq_256.pkl/projected_w.npz\" #@param {type: \"string\"}\n",
        "use_projected_w = True #@param {type:\"boolean\"}\n",
        "seed=2294 #@param {type: \"slider\", min: 0, max: 10000, step: 1}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# switch_layer = 128 #@param [4, 8, 16, 32, 64, 128]  {type:\"raw\"}\n",
        "\n",
        "blend_4 = 0 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_8 = 0.2 #@param {type: \"slider\", min: 0, max: 1,  step: 0.01}\n",
        "blend_16 = 0.47 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_32 = 1 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_64 = 1 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_128 = 1 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_256 = 1 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "\n",
        "blend_mask = [blend_4, blend_8, blend_16, blend_32, blend_64, blend_128, blend_256]\n",
        "print(blend_mask)\n",
        "\n",
        "model_out = blend_models(lo_G_ema, hi, model_res, model_res, level, blend_width=blend_width, blend_mask=blend_mask)\n",
        "\n",
        "G1 = lo_G_ema.to(device)\n",
        "G2 = hi.to(device)\n",
        "G_blend = model_out.to(device)\n",
        "\n",
        "\n",
        "if use_projected_w:\n",
        "  w_np = np.load(projected_w_path)['w']\n",
        "  w = torch.tensor(w_np).to(device)\n",
        "else:\n",
        "  label = torch.zeros([1, G1.c_dim], device=device)\n",
        "  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G1.z_dim)).to(device)\n",
        "\n",
        "  w = G1.mapping(z, None, truncation_psi=0.8, truncation_cutoff=8)\n",
        "\n",
        "\n",
        "g1_img = G1.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g1_img = (g1_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g1_imgfile = PIL.Image.fromarray(g1_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "# g1_imgfile.save(f'G1seed{seed:04d}.png')\n",
        "g2_img = G2.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g2_img = (g2_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g2_imgfile = PIL.Image.fromarray(g2_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "g3_img = G_blend.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g3_img = (g3_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g3_imgfile = PIL.Image.fromarray(g3_img[0].cpu().numpy(), 'RGB')\n",
        "display(add_imgs([g1_imgfile, g3_imgfile, g2_imgfile]))"
      ],
      "metadata": {
        "id": "mPO7EF_8IzOY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimental Overblending\n",
        "Choose values outside the 0-1 scale"
      ],
      "metadata": {
        "id": "AcAyqJMbRrsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Blend Layer {run: \"auto\"}\n",
        "device = \"cuda\"\n",
        "\n",
        "#@markdown **Select source vector**\n",
        "projected_w_path = \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/out/256_styglegan2_ada_3/projected_w.npz\" #@param {type: \"string\"}\n",
        "use_projected_w = False #@param {type:\"boolean\"}\n",
        "seed=2973 #@param {type: \"slider\", min: 0, max: 10000, step: 1}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# switch_layer = 128 #@param [4, 8, 16, 32, 64, 128]  {type:\"raw\"}\n",
        "\n",
        "blend_4 = -0.78 #@param {type: \"slider\", min: -10, max: 10, step: 0.01}\n",
        "blend_8 = 0.86 #@param {type: \"slider\", min: -10, max: 10, step: 0.01}\n",
        "blend_16 = 0.79 #@param {type: \"slider\", min: -10, max: 10, step: 0.01}\n",
        "blend_32 = -0.19 #@param {type: \"slider\", min: -10, max: 10, step: 0.01}\n",
        "blend_64 = 0.16 #@param {type: \"slider\", min: -10, max: 10, step: 0.01}\n",
        "blend_128 = 0.01 #@param {type: \"slider\", min: -10, max: 10, step: 0.01}\n",
        "blend_256 = 0.19 #@param {type: \"slider\", min: -10, max: 10, step: 0.01}\n",
        "\n",
        "blend_mask = [blend_4, blend_8, blend_16, blend_32, blend_64, blend_128, blend_256]\n",
        "print(blend_mask)\n",
        "\n",
        "model_out = blend_models(lo_G_ema, hi, model_res, model_res, level, blend_width=blend_width, blend_mask=blend_mask)\n",
        "\n",
        "G1 = lo_G_ema.to(device)\n",
        "G2 = hi.to(device)\n",
        "G_blend = model_out.to(device)\n",
        "\n",
        "\n",
        "if use_projected_w:\n",
        "  w_np = np.load(projected_w_path)['w']\n",
        "  w = torch.tensor(w_np).to(device)\n",
        "else:\n",
        "  label = torch.zeros([1, G1.c_dim], device=device)\n",
        "  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G1.z_dim)).to(device)\n",
        "\n",
        "  w = G1.mapping(z, None, truncation_psi=0.8, truncation_cutoff=8)\n",
        "\n",
        "\n",
        "g1_img = G1.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g1_img = (g1_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g1_imgfile = PIL.Image.fromarray(g1_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "# g1_imgfile.save(f'G1seed{seed:04d}.png')\n",
        "g2_img = G2.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g2_img = (g2_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g2_imgfile = PIL.Image.fromarray(g2_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "g3_img = G_blend.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g3_img = (g3_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g3_imgfile = PIL.Image.fromarray(g3_img[0].cpu().numpy(), 'RGB')\n",
        "display(add_imgs([g1_imgfile, g3_imgfile, g2_imgfile]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "M5tIuUI4RqzW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}