{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of network_blending_gui.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "wzJOHEPK-Zh4",
        "O09iXoAPQsHX",
        "5CyFcloD-3Lb",
        "IuJYW-SSICO3",
        "GHpwkbAKA4Dk",
        "AcAyqJMbRrsW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamdavidcole/stylegan2-ada-pytorch-adam/blob/main/network_blending_gui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdkWE1TKUWwB"
      },
      "source": [
        "# StyleGAN2 Network Blending\n",
        "\n",
        "A user interface for experimenting with StyleGAN2-ADA network blending. If you're looking for StyleGAN3 blending, check out [this notebook](https://github.com/adamdavidcole/stylegan3-fun-blend/blob/main/blend.ipynb).\n",
        "\n",
        "Select your source and destination models and play with various blend settings and sliders.\n",
        "\n",
        "This notebook supports models of 256x256 pixels but could be extended to support larger outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup libraries and Google drive connection"
      ],
      "metadata": {
        "id": "wzJOHEPK-Zh4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJFXX8WIBeqy"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops ninja gdown opensimplex torch==1.7.1 torchvision==0.8.2"
      ],
      "metadata": {
        "id": "WtrPIFTpz4OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive \n",
        "# (NOTE: only run this if you want to save the results in GDrive after the runtime ends)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gNwryEcAy2Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip install gdown --upgrade\n",
        "\n",
        "if os.path.isdir(\"/content/drive/MyDrive/stylegan2-ada-pytorch-adam\"):\n",
        "    %cd \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !git clone https://github.com/adamdavidcole/stylegan2-ada-pytorch-adam.git\n",
        "    %cd stylegan2-ada-pytorch-adam\n",
        "    \n",
        "    # !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "else:\n",
        "    !git clone https://github.com/adamdavidcole/stylegan2-ada-pytorch-adam.git\n",
        "    %cd stylegan2-ada-pytorch-adam\n",
        "    # !mkdir downloads\n",
        "    # !mkdir datasets\n",
        "    # !mkdir pretrained\n",
        "\n",
        "!mkdir pretrained\n",
        "!mkdir datasets\n",
        "!mkdir input_images\n",
        "!mkdir input_images/raw\n",
        "!mkdir input_images/aligned"
      ],
      "metadata": {
        "id": "Ypk368bp0rPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O09iXoAPQsHX"
      },
      "source": [
        "## Download Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRQUzZK2HHSt"
      },
      "source": [
        "# Download Various Pretrained Models\n",
        "# Uncomment the group you'd like to download\n",
        "# Mmake sure to use correct original and fine-tuned pair\n",
        "\n",
        "if not os.path.isdir('pretrained'):\n",
        "  !mkdir pretrained\n",
        "\n",
        "# FFHQ 256\n",
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl -O pretrained/ffhq_256.pkl\n",
        "\n",
        "#  Butterflys (Trained form FFHQ_256)\n",
        "!gdown --id 15NC-plFvfs59NLT0-t3SIucpJcEmvOmq -O pretrained/butterflys_000677.pkl\n",
        "\n",
        "#  Pok√©mon (Trained form FFHQ_256)\n",
        "!gdown --id 10sMVL02HibOs6fUdQSvUMQcs9h8mdsqz -O pretrained/pokemon_0503.pkl\n",
        "\n",
        "# # Ukiyoe Face (Trained form FFHQ_256_SLIM)\n",
        "!gdown --id 1BkRsnE0YygA2ufbfDOV4-fOgTMjSr94K -O pretrained/stylegan2-ffhq-slim.pkl\n",
        "!gdown --id 1BjYGiOUKk8SC35a2e5QrJ1QtvaxJ0QD7 -O pretrained/ukiyoe-256-slim.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Blend Functions and Utilities"
      ],
      "metadata": {
        "id": "5CyFcloD-3Lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#common functions \n",
        "import pickle, torch, PIL, copy, cv2, math\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "from IPython.display import Image as DisplayImage, clear_output\n",
        "\n",
        "# define device to use\n",
        "device = torch.device('cuda')\n",
        "\n",
        "def get_model(path):\n",
        "  # with open(path, 'rb') as f:\n",
        "  #   _G = pickle.load(f)['G_ema'].cuda()\n",
        "  device = torch.device('cuda')\n",
        "  with dnnlib.util.open_url(path) as fp:\n",
        "      _G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)\n",
        "  \n",
        "  return _G\n",
        "\n",
        "#tensor to PIL image \n",
        "def t2i(t):\n",
        "  return PIL.Image.fromarray((t*127.5+127).clamp(0,255)[0].permute(1,2,0).cpu().numpy().astype('uint8'))\n",
        "\n",
        "#stack an array of PIL images horizontally\n",
        "def add_imgs(images):\n",
        "  widths, heights = zip(*(i.size for i in images))\n",
        "\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    new_im.paste(im, (x_offset,0))\n",
        "    x_offset += im.size[0]\n",
        "  return new_im\n",
        "\n",
        "\n",
        "def apply_mask(matrix, mask, fill_value):\n",
        "    masked = np.ma.array(matrix, mask=mask, fill_value=fill_value)\n",
        "    return masked.filled()\n",
        " \n",
        "def apply_threshold(matrix, low_value, high_value):\n",
        "    low_mask = matrix < low_value\n",
        "    matrix = apply_mask(matrix, low_mask, low_value)\n",
        " \n",
        "    high_mask = matrix > high_value\n",
        "    matrix = apply_mask(matrix, high_mask, high_value)\n",
        " \n",
        "    return matrix\n",
        "\n",
        "# A simple color correction script to brighten overly dark images\n",
        "def simplest_cb(img, percent):\n",
        "    assert img.shape[2] == 3\n",
        "    assert percent > 0 and percent < 100\n",
        " \n",
        "    half_percent = percent / 200.0\n",
        " \n",
        "    channels = cv2.split(img)\n",
        " \n",
        "    out_channels = []\n",
        "    for channel in channels:\n",
        "        assert len(channel.shape) == 2\n",
        "        # find the low and high precentile values (based on the input percentile)\n",
        "        height, width = channel.shape\n",
        "        vec_size = width * height\n",
        "        flat = channel.reshape(vec_size)\n",
        " \n",
        "        assert len(flat.shape) == 1\n",
        " \n",
        "        flat = np.sort(flat)\n",
        " \n",
        "        n_cols = flat.shape[0]\n",
        " \n",
        "        low_val  = flat[math.floor(n_cols * half_percent)-1]\n",
        "        high_val = flat[math.ceil( n_cols * (1.0 - half_percent))-1]\n",
        " \n",
        " \n",
        "        # saturate below the low percentile and above the high percentile\n",
        "        thresholded = apply_threshold(channel, low_val, high_val)\n",
        "        # scale the channel\n",
        "        normalized = cv2.normalize(thresholded, thresholded.copy(), 0, 255, cv2.NORM_MINMAX)\n",
        "        out_channels.append(normalized)\n",
        " \n",
        "    return cv2.merge(out_channels)\n",
        " \n",
        "def normalize(inf, thresh):\n",
        "    img = np.array(inf)\n",
        "    out_img = simplest_cb(img, thresh)\n",
        "    return PIL.Image.fromarray(out_img)\n",
        "\n",
        "def get_w_from_path(w_path):\n",
        "  projected_w_np = np.load(projected_w_path)[0]\n",
        "  w = torch.tensor(projected_w_np).to(device).unsqueeze(0)\n",
        "  return w\n",
        "\n",
        "def synthesize_tensor_from_w(G, w):\n",
        "  # print(w.shape)\n",
        "  # print(w)\n",
        "  return G.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "\n",
        "def synthesize_img_from_w(G, w):\n",
        "  tensor = synthesize_tensor_from_w(G, w)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_tensor_from_w_path(G, w_path):\n",
        "  w = get_w_from_path(w_path)\n",
        "  return synthesize_tensor_from_w(G, w)\n",
        "\n",
        "def synthesize_img_from_w_path(G, w_path):\n",
        "  tensor = synthesize_tensor_from_w_path(G, w_path)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_img_from_w_path(G, w_path):\n",
        "  tensor = synthesize_tensor_from_w_path(G, w_path)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_img_from_w_np(G, w_np):\n",
        "  w = torch.tensor(w_np).to(device).unsqueeze(0)\n",
        "  tensor = synthesize_img_from_w(G, w)\n",
        "  return t2i(tensor)\n",
        "\n",
        "\n",
        "\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ],
      "metadata": {
        "id": "UpQmiH8zJlHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENNGfqvKS59e"
      },
      "source": [
        "#@title Blend Functions\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "def extract_conv_names(model, model_res):\n",
        "    model_names = list(name for name,weight in model.named_parameters())\n",
        "\n",
        "    return model_names\n",
        "\n",
        "def blend_models(low, high, model_res, resolution, level, blend_width=None, blend_mask=None, should_invert=False):\n",
        "\n",
        "    resolutions =  [4*2**x for x in range(int(np.log2(resolution)-1))]\n",
        "    \n",
        "    low_names = extract_conv_names(low, model_res)\n",
        "    high_names = extract_conv_names(high, model_res)\n",
        "\n",
        "    assert all((x == y for x, y in zip(low_names, high_names)))\n",
        "\n",
        "    #start with lower model and add weights above\n",
        "    model_out = copy.deepcopy(low)\n",
        "    params_src = high.named_parameters()\n",
        "    dict_dest = model_out.state_dict()\n",
        "\n",
        "    if blend_mask is None:\n",
        "      for name, param in params_src:\n",
        "          if should_invert: \n",
        "            if any(f'synthesis.b{res}' in name for res in resolutions) and not ('mapping' in name):\n",
        "              dict_dest[name].data.copy_(param.data)\n",
        "          else:\n",
        "            if not any(f'synthesis.b{res}' in name for res in resolutions) and not ('mapping' in name):\n",
        "              dict_dest[name].data.copy_(param.data)\n",
        "\n",
        "    else:\n",
        "      for name, param in params_src:\n",
        "        if not ('mapping' in name):\n",
        "          # print(f\"name: {name}\")\n",
        "\n",
        "\n",
        "          for idx, res in enumerate(resolutions):\n",
        "            if f'synthesis.b{res}' in name:\n",
        "              mask_val = blend_mask[idx]\n",
        "              next_data = dict_dest[name].data * (1 - mask_val) + param.data * (mask_val)\n",
        "\n",
        "              # print(mask_val)\n",
        "\n",
        "              dict_dest[name].data.copy_(next_data)\n",
        "\n",
        "\n",
        "    model_out_dict = model_out.state_dict()\n",
        "    model_out_dict.update(dict_dest) \n",
        "    model_out.load_state_dict(dict_dest)\n",
        "    \n",
        "    return model_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select Models To Blend\n",
        "\n",
        "**Select a source and destination model.** \n",
        "\n",
        "Keep in mind that the destination model needs to be fine-tuned from the source model for the blend to work. \n",
        "\n",
        "Feel free to paste in links to other pairs of models you'd like to expriment with."
      ],
      "metadata": {
        "id": "EGy4JG7p-9Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-vfD9EeQ_JOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title {run: \"auto\"}\n",
        "#@markdown Select a pretrained model for the source and destination or paste links to your own\n",
        "#@markdown <br/>(Note: destination must be fine-tuned from source and both must be StyleGAN2 pkl format)\n",
        "source_model = \"FFHQ_256\" #@param [\"FFHQ_256\", \"FFHQ_256_slim\"] {allow-input: true}\n",
        "destination_model = \"Butteflys\" #@param [\"Butteflys\", \"Pokemon\", \"Ukiyoe_256_slim\"] {allow-input: true}\n",
        "\n",
        "\n",
        "model_keys = {\n",
        "    \"FFHQ_256\": \"pretrained/ffhq_256.pkl\", \n",
        "    \"Butteflys\": \"pretrained/butterflys_000677.pkl\",\n",
        "    \"Pokemon\": \"pretrained/pokemon_0503.pkl\",\n",
        "\n",
        "    \"FFHQ_256_slim\": \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/pretrained/stylegan2-ffhq-slim.pkl\",\n",
        "    \"Ukiyoe_256_slim\": \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/pretrained/ukiyoe-256-slim.pkl\"\n",
        "}\n",
        "\n",
        "lo_res_pkl = model_keys[source_model] if source_model in model_keys  else source_model\n",
        "hi_res_pkl = model_keys[destination_model] if destination_model in model_keys else destination_model\n",
        "model_res = 256\n",
        "level = 0\n",
        "blend_width=None\n",
        "\n",
        "G_kwargs = dnnlib.EasyDict()\n",
        "\n",
        "with dnnlib.util.open_url(lo_res_pkl) as f:\n",
        "    # G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "    lo = legacy.load_network_pkl(f, custom=False, **G_kwargs) # type: ignore\n",
        "    lo_G, lo_D, lo_G_ema = lo['G'], lo['D'], lo['G_ema']\n",
        "\n",
        "with dnnlib.util.open_url(hi_res_pkl) as f:\n",
        "    # G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "    hi = legacy.load_network_pkl(f, custom=False, **G_kwargs)['G_ema'] # type: ignore\n",
        "    #hi_G, hi_D, hi_G_ema = hi['G'], lo['D'], lo['G_ema']\n"
      ],
      "metadata": {
        "id": "hQbd5bciK4AH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Training\n",
        "Optional: create a new source/fine-tuned set of models from the source model (only necessary if not using a pretrained models)"
      ],
      "metadata": {
        "id": "IuJYW-SSICO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Training Settings\n",
        "# (trained models output to the \"results\" folder)\n",
        "\n",
        "# path to source model\n",
        "resume_network = lo_res_pkl\n",
        "# path to zip with prepped dataset\n",
        "data_zip = \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/input_images/raw/pokemon_256.zip\"\n",
        "# training config to use (for 256 images use \"paper256\")\n",
        "cfg=\"paper256\"\n",
        "# Network snapshot frequenct (to see updates frequently use snap=1)\n",
        "snap=1\n",
        "\n",
        "!python train.py --outdir=results --data=$data_zip \\\n",
        "  --gpus=1 --cfg=$cfg --mirror=1 --snap=$snap --aug=noaug --metrics=none --resume=$resume_network\n"
      ],
      "metadata": {
        "id": "-QS7R8hcIDwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Image Pojection"
      ],
      "metadata": {
        "id": "GHpwkbAKA4Dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Face\n",
        "To project a custom face into the FFHQ latent space:\n",
        "\n",
        "\n",
        "1.   Upload a square high res image to `input_images/raw`\n",
        "2.   Run the `align_images.py` script below\n",
        "3.   The aligned image will be output to `input_images/aligned`. Copy the path of the aligned image into the `uploaded_file_path` form option below\n",
        "4.   Select a number of steps (5000 steps usually provides a high quality projectiob, but takes more time, 1000 is a good starting spot for testing)\n",
        "5.   Run the `projector.py` cell below\n",
        "6. Copy the path of the projected `.npy` file into the blending GUI forms below\n",
        "\n"
      ],
      "metadata": {
        "id": "ypk8JJhaE5au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###¬†Align Face Images\n",
        "\n",
        "# Upload a file to input_images/raw\n",
        "# Aligned image will be output to input_images/aligned\n",
        "!rm -r -f input_images/raw/.ipynb_checkpoints\n",
        "!python align_images.py "
      ],
      "metadata": {
        "id": "MxHR8i2cUVOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Project Image Into Latent Space\n",
        "\n",
        "import time \n",
        "# ts stores the time in seconds\n",
        "ts = int(time.time())\n",
        "\n",
        "num_steps = 5011 #@param {type: \"slider\", min: 1, max: 10000, step: 1}\n",
        "uploaded_file_path = \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/input_images/aligned/corey_miller_01.png\" #@param {type: \"string\"} \n",
        "\n",
        "uploaded_file_name_with_ext = os.path.basename(uploaded_file_path)\n",
        "uploaded_file_name = os.path.splitext(uploaded_file_name_with_ext)[0]\n",
        "\n",
        "\n",
        "network_name_with_ext = os.path.basename(lo_res_pkl)\n",
        "network_name = os.path.splitext(network_name_with_ext)[0]\n",
        "projection_outdir = f\"projections/{ts}__{network_name}__{uploaded_file_name}__{num_steps}\"\n",
        "\n",
        "\n",
        "!python projector.py --outdir=$projection_outdir --target=$uploaded_file_path --num-steps=$num_steps --save-video=false \\\n",
        "  --network=$lo_res_pkl"
      ],
      "metadata": {
        "id": "u_kr75qaE8xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Blending"
      ],
      "metadata": {
        "id": "vlvt31V4A-Hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network Blend Basic\n",
        "\n",
        "Simple blend functions between the source and desination models. Select a seed value and blend mode."
      ],
      "metadata": {
        "id": "TgF1sYKLFYRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Blend Layer {run: \"auto\"}\n",
        "device = \"cuda\"\n",
        "\n",
        "#@markdown **Optional: select source vector**\n",
        "projected_w_path = \"\" #@param {type: \"string\"}\n",
        "use_projected_w = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown **Select seed**\n",
        "seed=1414 #@param {type: \"slider\", min: 0, max: 10000, step: 1}\n",
        "truncation_psi = 0.8 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Select blend layer options**\n",
        "switch_layer = 16 #@param [4, 8, 16, 32, 64, 128]  {type:\"raw\"}\n",
        "should_invert = False #@param {type: \"boolean\"}\n",
        "\n",
        "# blend_width = 0 #@param {type: \"slider\", min: 0, max: 5, step: 0.01}\n",
        "\n",
        "# switch_layer_inversion_map = {\n",
        "#     4: 128,\n",
        "#     8: 64,\n",
        "#     16: 32,\n",
        "#     32: 16,\n",
        "#     64: 8,\n",
        "#     128: 4\n",
        "# }\n",
        "# if invert_switch_layer:\n",
        "#   switch_layer = switch_layer_inversion_map[switch_layer]\n",
        "\n",
        "model_out = blend_models(lo_G_ema, hi, model_res, switch_layer, level, blend_width=blend_width, should_invert=should_invert)\n",
        "\n",
        "G1 = lo_G_ema.to(device)\n",
        "G2 = hi.to(device)\n",
        "G_blend = model_out.to(device)\n",
        "\n",
        "\n",
        "if use_projected_w:\n",
        "  w_np = np.load(projected_w_path)['w']\n",
        "  w = torch.tensor(w_np).to(device)\n",
        "else:\n",
        "  label = torch.zeros([1, G1.c_dim], device=device)\n",
        "  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G1.z_dim)).to(device)\n",
        "\n",
        "  w = G1.mapping(z, None, truncation_psi=truncation_psi, truncation_cutoff=8)\n",
        "\n",
        "\n",
        "g1_img = G1.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g1_img = (g1_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g1_imgfile = PIL.Image.fromarray(g1_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "# g1_imgfile.save(f'G1seed{seed:04d}.png')\n",
        "g2_img = G2.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g2_img = (g2_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g2_imgfile = PIL.Image.fromarray(g2_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "g3_img = G_blend.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g3_img = (g3_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g3_imgfile = PIL.Image.fromarray(g3_img[0].cpu().numpy(), 'RGB')\n",
        "display(add_imgs([g1_imgfile, g3_imgfile, g2_imgfile]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1OHi3Tm5of-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tune Blening\n",
        "\n",
        "Fine tuned blending allows you to control the individual blend levels between the source and destination models. Higher sliders will control coarser, structual behavior while lower sliders control finer details like color and texture."
      ],
      "metadata": {
        "id": "0mcnplXLoMkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Blend Layer {run: \"auto\"}\n",
        "device = \"cuda\"\n",
        "\n",
        "#@markdown **Optional: Select source vector**\n",
        "projected_w_path = \"\" #@param {type: \"string\"}\n",
        "use_projected_w = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown **Select seed options**\n",
        "\n",
        "seed=2271 #@param {type: \"slider\", min: 0, max: 10000, step: 1}\n",
        "truncation_psi = 0.8 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "#@markdown ---\n",
        "#@markdown **Select blend amount for each layer**\n",
        "\n",
        "# switch_layer = 128 #@param [4, 8, 16, 32, 64, 128]  {type:\"raw\"}\n",
        "\n",
        "blend_4 = 0.79 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_8 = 0.79 #@param {type: \"slider\", min: 0, max: 1,  step: 0.01}\n",
        "blend_16 = 0.79 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_32 = 0.36 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_64 = 0.08 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_128 = 0.07 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "blend_256 = 0.08 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "\n",
        "blend_mask = [blend_4, blend_8, blend_16, blend_32, blend_64, blend_128, blend_256]\n",
        "print(blend_mask)\n",
        "\n",
        "model_out = blend_models(lo_G_ema, hi, model_res, model_res, level, blend_width=blend_width, blend_mask=blend_mask)\n",
        "\n",
        "G1 = lo_G_ema.to(device)\n",
        "G2 = hi.to(device)\n",
        "G_blend = model_out.to(device)\n",
        "\n",
        "\n",
        "if use_projected_w:\n",
        "  w_np = np.load(projected_w_path)['w']\n",
        "  w = torch.tensor(w_np).to(device)\n",
        "else:\n",
        "  label = torch.zeros([1, G1.c_dim], device=device)\n",
        "  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G1.z_dim)).to(device)\n",
        "\n",
        "  print(f\"Seed: {seed}\")\n",
        "\n",
        "  w = G1.mapping(z, None, truncation_psi=truncation_psi, truncation_cutoff=8)\n",
        "\n",
        "\n",
        "g1_img = G1.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g1_img = (g1_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g1_imgfile = PIL.Image.fromarray(g1_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "# g1_imgfile.save(f'G1seed{seed:04d}.png')\n",
        "g2_img = G2.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g2_img = (g2_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g2_imgfile = PIL.Image.fromarray(g2_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "g3_img = G_blend.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g3_img = (g3_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g3_imgfile = PIL.Image.fromarray(g3_img[0].cpu().numpy(), 'RGB')\n",
        "display(add_imgs([g1_imgfile, g3_imgfile, g2_imgfile]))"
      ],
      "metadata": {
        "id": "mPO7EF_8IzOY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimental: Overblending\n",
        "Overblending is a technique where you go beyond the sensible values of 0 and 1 when blending between the source and destination. The results often collapse when the sliders go far beyond the [0-1] scale, but it's fun to play with!"
      ],
      "metadata": {
        "id": "AcAyqJMbRrsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Blend Layer {run: \"auto\"}\n",
        "device = \"cuda\"\n",
        "\n",
        "#@markdown **Select source vector**\n",
        "projected_w_path = \"/content/drive/MyDrive/stylegan2-ada-pytorch-adam/projections/1655560119_ffhq_256/projected_w.npz\" #@param {type: \"string\"}\n",
        "use_projected_w = False #@param {type:\"boolean\"}\n",
        "seed=2498 #@param {type: \"slider\", min: 0, max: 10000, step: 1}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# switch_layer = 128 #@param [4, 8, 16, 32, 64, 128]  {type:\"raw\"}\n",
        "\n",
        "blend_4 = 1.23 #@param {type: \"slider\", min: -3, max: 3, step: 0.01}\n",
        "blend_8 = 1.01 #@param {type: \"slider\", min: -3, max: 3, step: 0.01}\n",
        "blend_16 = 0.94 #@param {type: \"slider\", min: -3, max: 3, step: 0.01}\n",
        "blend_32 = -0.14 #@param {type: \"slider\", min: -3, max: 3, step: 0.01}\n",
        "blend_64 = -0.14 #@param {type: \"slider\", min: -3, max: 3, step: 0.01}\n",
        "blend_128 = -0.2 #@param {type: \"slider\", min: -3, max: 3, step: 0.01}\n",
        "blend_256 = -0.13 #@param {type: \"slider\", min: -3, max: 3, step: 0.01}\n",
        "\n",
        "blend_mask = [blend_4, blend_8, blend_16, blend_32, blend_64, blend_128, blend_256]\n",
        "model_out = blend_models(lo_G_ema, hi, model_res, model_res, level, blend_width=blend_width, blend_mask=blend_mask)\n",
        "\n",
        "blend_mask_clipped = np.clip(blend_mask, 0, 1)\n",
        "model_out_clipped = blend_models(lo_G_ema, hi, model_res, model_res, level, blend_width=blend_width, blend_mask=blend_mask_clipped)\n",
        "\n",
        "G1 = lo_G_ema.to(device)\n",
        "G2 = hi.to(device)\n",
        "G_blend = model_out.to(device)\n",
        "G_blend_clipped = model_out_clipped.to(device)\n",
        "\n",
        "\n",
        "if use_projected_w:\n",
        "  w_np = np.load(projected_w_path)['w']\n",
        "  w = torch.tensor(w_np).to(device)\n",
        "\n",
        "  print(f\"W: {'/'.join(projected_w_path.split('/')[-2:])}\")\n",
        "else:\n",
        "  label = torch.zeros([1, G1.c_dim], device=device)\n",
        "  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G1.z_dim)).to(device)\n",
        "  print(f\"Seed: {seed}\")\n",
        "  w = G1.mapping(z, None, truncation_psi=0.8, truncation_cutoff=8)\n",
        "\n",
        "print(blend_mask)\n",
        "\n",
        "\n",
        "g1_img = G1.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g1_img = (g1_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g1_imgfile = PIL.Image.fromarray(g1_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "# g1_imgfile.save(f'G1seed{seed:04d}.png')\n",
        "g2_img = G2.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g2_img = (g2_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g2_imgfile = PIL.Image.fromarray(g2_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "g3_img = G_blend.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g3_img = (g3_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g3_imgfile = PIL.Image.fromarray(g3_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "g4_img = G_blend_clipped.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "g4_img = (g4_img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "g4_imgfile = PIL.Image.fromarray(g4_img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "\n",
        "\n",
        "display(add_imgs([g1_imgfile, g3_imgfile, g4_imgfile, g2_imgfile]))"
      ],
      "metadata": {
        "id": "M5tIuUI4RqzW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Refrences\n"
      ],
      "metadata": {
        "id": "msDvJzq99e_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was created by [Adam Cole](https://www.instagram.com/adamcole.studio/) with a specific focus on building a user interface around network blending for artists to experiment with.\n",
        "\n",
        "### Sources\n",
        "- This code lives in a [fork of StyleGAN2](https://github.com/dvschultz/stylegan2-ada-pytorch) by [@dvschultz](https://github.com/dvschultz) and we take advantage of the training, projection, blending and utility function in that repo.\n",
        "- The idea to use a \"blend mask\" and many helper functions were taken fully from [@Sxela](https://github.com/PDillis) [stylegan3_blending](https://github.com/Sxela/stylegan3_blending) repo\n",
        "- Much of this work and some of the models were taken from Justin Pinkney's blogpost on network blending."
      ],
      "metadata": {
        "id": "vFUOzvwQCLPs"
      }
    }
  ]
}